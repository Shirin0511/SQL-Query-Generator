{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Required Imports"
      ],
      "metadata": {
        "id": "f-UBOIEr-L-J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FcLLBCXv5Dqh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from transformers import T5Tokenizer\n",
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import sqlparse\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting data from Github"
      ],
      "metadata": {
        "id": "HjtU599i-QAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/salesforce/WikiSQL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMGdNjcdRK_e",
        "outputId": "e9aac352-25ab-4ca9-fe7c-21dce5bd9247"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WikiSQL'...\n",
            "remote: Enumerating objects: 389, done.\u001b[K\n",
            "remote: Counting objects: 100% (195/195), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 389 (delta 186), reused 154 (delta 154), pack-reused 194 (from 1)\u001b[K\n",
            "Receiving objects: 100% (389/389), 50.72 MiB | 55.66 MiB/s, done.\n",
            "Resolving deltas: 100% (213/213), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd WikiSQL && tar -xvf data.tar.bz2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKPFnOXDrxtk",
        "outputId": "83a900cc-11f4-4b3c-d2f8-b032e1ea7595"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/\n",
            "data/train.jsonl\n",
            "data/test.tables.jsonl\n",
            "data/test.db\n",
            "data/dev.tables.jsonl\n",
            "data/dev.db\n",
            "data/test.jsonl\n",
            "data/train.tables.jsonl\n",
            "data/train.db\n",
            "data/dev.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls WikiSQL/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNNlztbIsK7d",
        "outputId": "674a7e8e-f812-442f-9f12-23049835a800"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev.db\t   dev.tables.jsonl  test.jsonl\t\ttrain.db     train.tables.jsonl\n",
            "dev.jsonl  test.db\t     test.tables.jsonl\ttrain.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"WikiSQL/data/train.jsonl\") as f:\n",
        "    sample = json.loads(next(f))\n",
        "\n",
        "with open(\"WikiSQL/data/train.tables.jsonl\") as f:\n",
        "    table = json.loads(next(f))\n",
        "\n",
        "print(\"QUESTION:\\n\", sample[\"question\"])\n",
        "print(\"\\nSQL LABEL:\\n\", sample[\"sql\"])\n",
        "print(\"\\nTABLE SCHEMA:\\n\", table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkFRgQZEsgkG",
        "outputId": "81dbffae-c8a3-46cb-b4c2-1c9fe312ff6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION:\n",
            " Tell me what the notes are for South Australia \n",
            "\n",
            "SQL LABEL:\n",
            " {'sel': 5, 'conds': [[3, 0, 'SOUTH AUSTRALIA']], 'agg': 0}\n",
            "\n",
            "TABLE SCHEMA:\n",
            " {'id': '1-1000181-1', 'header': ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes'], 'types': ['text', 'text', 'text', 'text', 'text', 'text'], 'rows': [['Australian Capital Territory', 'blue/white', 'Yaa·nna', 'ACT · CELEBRATION OF A CENTURY 2013', 'YIL·00A', 'Slogan screenprinted on plate'], ['New South Wales', 'black/yellow', 'aa·nn·aa', 'NEW SOUTH WALES', 'BX·99·HI', 'No slogan on current series'], ['New South Wales', 'black/white', 'aaa·nna', 'NSW', 'CPX·12A', 'Optional white slimline series'], ['Northern Territory', 'ochre/white', 'Ca·nn·aa', 'NT · OUTBACK AUSTRALIA', 'CB·06·ZZ', 'New series began in June 2011'], ['Queensland', 'maroon/white', 'nnn·aaa', 'QUEENSLAND · SUNSHINE STATE', '999·TLG', 'Slogan embossed on plate'], ['South Australia', 'black/white', 'Snnn·aaa', 'SOUTH AUSTRALIA', 'S000·AZD', 'No slogan on current series'], ['Victoria', 'blue/white', 'aaa·nnn', 'VICTORIA - THE PLACE TO BE', 'ZZZ·562', 'Current series will be exhausted this year']], 'name': 'table_1000181_1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting WikiSQL into SQL String"
      ],
      "metadata": {
        "id": "mizTaQ2e_bVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AGG_OPS = [\"\", \"MAX\", \"MIN\", \"COUNT\", \"SUM\", \"AVG\"]\n",
        "COND_OPS = [\"=\", \">\", \"<\", \"OP\"]\n",
        "\n",
        "def convert_sql(sample, table):\n",
        "  col_names= table['header']\n",
        "\n",
        "  select_col= col_names[sample['sql']['sel']]\n",
        "\n",
        "  agg= AGG_OPS[sample['sql']['agg']]\n",
        "\n",
        "  if agg:\n",
        "    select_part = f\"{agg}({select_col})\"\n",
        "  else:\n",
        "    select_part = select_col\n",
        "\n",
        "\n",
        "  where_clause=[]\n",
        "\n",
        "  for c in sample['sql']['conds']:\n",
        "    column= col_names[c[0]]\n",
        "    op= COND_OPS[c[1]]\n",
        "    value= c[2]\n",
        "    where_clause.append(f\"{column}{op}'{value}'\")\n",
        "\n",
        "  where = \" AND \".join(where_clause)\n",
        "\n",
        "  query = f\"SELECT {select_part} FROM table\"\n",
        "\n",
        "  if where:\n",
        "    query += f\" WHERE {where}\"\n",
        "\n",
        "  return query"
      ],
      "metadata": {
        "id": "ow1YczGNsxPv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# schema-aware input\n",
        "def build_schema(table):\n",
        "  return \", \".join([\n",
        "  f\"{col}({typ})\"\n",
        "  for col, typ in zip(table['header'], table['types'])\n",
        "  ])"
      ],
      "metadata": {
        "id": "PH6WDXS30gNb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Training Examples"
      ],
      "metadata": {
        "id": "jkXaqeDP_mwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_pairs=[]\n",
        "\n",
        "with open(\"WikiSQL/data/train.jsonl\") as f_data, \\\n",
        "     open(\"WikiSQL/data/train.tables.jsonl\") as f_tables:\n",
        "\n",
        "     tables={}\n",
        "\n",
        "     for t in f_tables:\n",
        "      table_obj= json.loads(t)\n",
        "      tables[table_obj['id']]= table_obj\n",
        "\n",
        "     for line in f_data:\n",
        "        sample = json.loads(line)\n",
        "        actual_table = tables[sample['table_id']]\n",
        "\n",
        "        schema= build_schema(actual_table)\n",
        "\n",
        "        sql = convert_sql(sample, actual_table)\n",
        "\n",
        "        inp= f\"Schema: {schema}, Question: {sample['question']}\"\n",
        "\n",
        "        training_pairs.append((inp,sql))\n",
        "\n",
        "     training_pairs= training_pairs[:8000]\n"
      ],
      "metadata": {
        "id": "IP0bBcMi2-Jq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbfhA6MTGH7L",
        "outputId": "56ae1041-7ada-4b45-c536-f5d5eacba513"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "MT0RYoN6_4FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize for T5\n",
        "tokenizer= T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "def tokenize(example):\n",
        "  return tokenizer(\n",
        "      example[0],\n",
        "      text_target = example[1],\n",
        "      truncation= True,\n",
        "      padding= \"max_length\",\n",
        "      max_length= 256\n",
        "  )"
      ],
      "metadata": {
        "id": "3Bb6zJRv8aHA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting training pairs into tokenized data\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, pairs, tokenizer):\n",
        "        self.pairs = pairs\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, out = self.pairs[idx]\n",
        "        tokens = self.tokenizer(\n",
        "            inp,\n",
        "            text_target=out,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=256\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokens[\"input_ids\"],\n",
        "            \"attention_mask\": tokens[\"attention_mask\"],\n",
        "            \"labels\": tokens[\"labels\"]\n",
        "        }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WOZEdAfhCNEy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data= SQLDataset(training_pairs,tokenizer)"
      ],
      "metadata": {
        "id": "JjL2Q9ugDLSE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning T5"
      ],
      "metadata": {
        "id": "emKXLW0o_83t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "args= TrainingArguments(\n",
        "    output_dir= './sql_model',\n",
        "    learning_rate= 3e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    save_steps=50\n",
        ")\n",
        "\n",
        "trainer= Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset= tokenized_data\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "tpOR7TBu-FQa",
        "outputId": "1938ac24-9e79-4f3d-9708-2201128eccd6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 14:27, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.115300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.027000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.0711275749206543, metrics={'train_runtime': 868.3336, 'train_samples_per_second': 18.426, 'train_steps_per_second': 1.152, 'total_flos': 1082734411776000.0, 'train_loss': 0.0711275749206543, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./sql_model\")\n",
        "tokenizer.save_pretrained(\"./sql_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4yAbV0NND_O",
        "outputId": "7f8b950d-9397-4a4b-bfb9-d601e9ef0af9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./sql_model/tokenizer_config.json',\n",
              " './sql_model/special_tokens_map.json',\n",
              " './sql_model/spiece.model',\n",
              " './sql_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_sql(sql):\n",
        "    return bool(sqlparse.parse(sql))"
      ],
      "metadata": {
        "id": "3J3dAfzBDio4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "K9Al_d9UMA-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"./sql_model\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./sql_model\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvzl_NZjLjHO",
        "outputId": "5e8a7df5-e4f7-4f92-a411-d47e55fe4349"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sql(schema, question, max_length=128):\n",
        "    input_text = f\"Schema: {schema}, Question: {question}\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            no_repeat_ngram_size=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return sql\n"
      ],
      "metadata": {
        "id": "9P0yAZFyMCwH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"Name(text), Age(number), Salary(number), Department(text)\"\n",
        "question = \"What is the maximum salary?\"\n",
        "sql= generate_sql(schema, question)\n",
        "\n",
        "if is_valid_sql(sql):\n",
        "    print(\"Generated SQL:\", sql)\n",
        "else:\n",
        "    print(\"Invalid SQL generated\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYDgCPtyN9oo",
        "outputId": "247b2455-f6e9-440e-f4ca-939c4eac4975"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL: SELECT MAX(Salary) FROM table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"Name(text), Age(number), Salary(number), Department(text)\"\n",
        "question = \"What is the salary of Matthew?\"\n",
        "\n",
        "sql= generate_sql(schema, question)\n",
        "\n",
        "if is_valid_sql(sql):\n",
        "    print(\"Generated SQL:\", sql)\n",
        "else:\n",
        "    print(\"Invalid SQL generated\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNV9nxRDUGfH",
        "outputId": "be88ad47-68d5-4c59-ee62-785c9cb688de"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL: SELECT Salary FROM table WHERE Name='Matthew'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"Name(text), Age(number), Salary(number), Department(text)\"\n",
        "question = \"How many records are there in Sales Department?\"\n",
        "\n",
        "sql= generate_sql(schema, question)\n",
        "\n",
        "if is_valid_sql(sql):\n",
        "    print(\"Generated SQL:\", sql)\n",
        "else:\n",
        "    print(\"Invalid SQL generated\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhuSAQogUMID",
        "outputId": "dd215ea3-0ed0-4c2e-8256-049b975191de"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL: SELECT COUNT(Name) FROM table WHERE Department='Sales Department'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ULZc9a82VFXE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDKPIDkDSiTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}